{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOVmTqhBoOoho4SZeo9wl6D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GRxS-Cw03TN","executionInfo":{"status":"ok","timestamp":1761454321137,"user_tz":-330,"elapsed":19862,"user":{"displayName":"Guna Sekhar","userId":"02678491187712962059"}},"outputId":"e098c046-ff96-4cb9-f022-e4d6025ecdce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading dataset...\n","\n"," Loaded 1400 reviews (700 positive / 700 negative)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Model Metrics:\n","\n","             Features  #Features  NaiveBayes    LogReg       SVM\n","    (1) Unigrams freq      12960   76.499465       NaN 76.642986\n","(2) Unigrams presence      12960   81.000542 82.143043 81.285899\n","     (4) Bigrams only      15825   78.571560 76.713905 74.642423\n"," (3) Unigrams+Bigrams      24462   81.001308 81.071920 79.571918\n","  (5) Adjectives only       1925   76.501457 71.716401 70.644972\n","(6) Top 2633 unigrams       2633   81.072532 80.928705 79.213805\n"]}],"source":["import re, zipfile, urllib.request\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import cross_val_score\n","# Fetch & Extract Dataset\n","url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip\"\n","zip_file = Path(\"mix20_rand700_tokens_cleaned.zip\")#reading file\n","data_dir = Path(zip_file.stem)# path means root directory address\n","print(\"Downloading dataset...\")\n","urllib.request.urlretrieve(url, zip_file)\n","with zipfile.ZipFile(zip_file, \"r\") as zf:\n","  zf.extractall(data_dir)\n","pos_path = data_dir / \"tokens\" / \"pos\"\n","neg_path = data_dir / \"tokens\" / \"neg\"\n","dataset_version = \"Polarity v0.9/v1.0 â€“ 700 pos / 700 neg\"\n","#Load Data\n","def read_reviews(pos_dir, neg_dir):\n","    docs, y = [], []\n","    for lbl, folder in [(1, pos_dir), (0, neg_dir)]:\n","        for file in Path(folder).glob(\"*.txt\"):\n","            text = file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n","            # strip ratings like \"10/10\" or \"****\"\n","            text = re.sub(r\"\\d+/\\d+|\\*+\", \"\", text)\n","            docs.append(text)\n","            y.append(lbl)\n","    return docs, np.array(y)\n","reviews, labels = read_reviews(pos_path, neg_path)\n","print(f\"\\n Loaded {len(reviews)} reviews \"\n","      f\"({labels.sum()} positive / {(labels==0).sum()} negative)\")\n","#Evaluation Helper\n","def cv_score(model, X, y, folds=3):\n","    return cross_val_score(model, X, y, cv=folds).mean() * 100\n","#Feature Experiments\n","experiments = []\n","token_rule = r\"(?u)\\b\\w+\\b\"\n","# (1) Unigrams (counts)\n","vec = CountVectorizer(binary=False, token_pattern=token_rule, min_df=4)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(1) Unigrams freq\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    None,\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","# (2) Unigrams (binary presence)\n","vec = CountVectorizer(binary=True, token_pattern=token_rule, min_df=4)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(2) Unigrams presence\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    cv_score(LogisticRegression(max_iter=1000), X, labels),\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","#  (3) Bigrams only\n","vec = CountVectorizer(binary=True, ngram_range=(2,2),\n","                      token_pattern=token_rule, min_df=7)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(4) Bigrams only\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    cv_score(LogisticRegression(max_iter=1000), X, labels),\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","#(4) Unigrams + Bigrams\n","vec = CountVectorizer(binary=True, ngram_range=(1,2),\n","                      token_pattern=token_rule, min_df=7)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(3) Unigrams+Bigrams\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    cv_score(LogisticRegression(max_iter=1000), X, labels),\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","# (5) Adjective-based tokens\n","def adjective_filter(text):\n","    words = re.findall(r\"\\b\\w+\\b\", text)\n","    pattern = r\"(ly$|ous$|ful$|able$|ive$|less$|ic$|al$|est$|er$)\"\n","    keywords = {\"good\", \"bad\", \"great\", \"awful\", \"excellent\", \"poor\"}\n","    return [w for w in words if re.search(pattern, w.lower()) or w.lower() in keywords]\n","vec = CountVectorizer(tokenizer=adjective_filter, binary=True, min_df=4)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(5) Adjectives only\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    cv_score(LogisticRegression(max_iter=1000), X, labels),\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","# (6) Top 2633 unigrams\n","vec = CountVectorizer(binary=True, token_pattern=token_rule, max_features=2633)\n","X = vec.fit_transform(reviews)\n","experiments.append([\"(6) Top 2633 unigrams\", X.shape[1],\n","                    cv_score(MultinomialNB(), X, labels),\n","                    cv_score(LogisticRegression(max_iter=1000), X, labels),\n","                    cv_score(LinearSVC(max_iter=5000), X, labels)])\n","#Printing Results\n","results_df = pd.DataFrame(experiments, columns=[\"Features\", \"#Features\", \"NaiveBayes\", \"LogReg\", \"SVM\"])\n","print(\"\\nModel Metrics:\\n\")\n","print(results_df.to_string(index=False))"]}]}